Module 9
Intro to Statistical Inference

Preliminaries: install the {curl} package
Objective: to continue the discussion of statistical inference and introduce basic hypothesis testing from a frequentist/classical statistics approach.

Standard Errors and Confidence Intervals (CIs)
  Standard errors: key to calculating CIs and basic inferential stats. Equals the value of the stat being considered (eg the mean) +/- some critcal value * the standard error of the stat. 
    The critical value is derived from the standardized version of a sampling distribution (eg the normal distribution) that corresponds to the quantile limits we're interested in. For 95% CI around the mean, the crit value corresponds to the range of quantiles above/below which we expect to see only 5% of the distribution of stat values; equivalent to the +/- 1-(alpha/2), where alpha=0.05, ie the +/- 0.975 quantile we used to calculate 95% CIs.
    The standard error is the standard deviation of the sampling distribution; can be estimated as sigma/sqrt(n) or found from the population standard deviation.
  Module 8 review:
    n<-1000
    mu<-3.5
    sigma<-4
    v<-rnorm(n, mu, sigma) [rnorm() always generated random numbers fitting a normal distribution pattern]
    s<-sample(v, size=30, replace=FLASE)
    m<-mean(s)
    sd<-sd(s)
    sem<-sd(s)/sqrt(length(s))
    lower<-m-qnorm(1-0.05/2)*sem
    upper<-m+qnorm(1-0.05/2)*sem
    ci<-c(lower, upper) [returns the CI values for this generate distribution]
    
The Central Limit Theorem
  Central Limit Theorem/CLT states the distribution of averages/sums/other summary stats of iid (independent and identically distributed) random variables becomes normal as the sample size increases. If our sample size is large enough, CLT allows us to make inferences about a population based on a sample.
  Simulation: an event occurring in a population according to a PMF like Poisson, lambda=14. For the Poisson distribution, mu and sigma squared are both equal to lambda. We'll take 1000 samples of size 10 from this population, calculate the averages, and plot a histogram of the averages. It will be close to normal, and the standard deviation of the averages (ie of the sampling distribution) should be roughly equal to the estimated standard error, the square root of (lambda/n). Since lambda is the expected variance, this is the square root of expected variance/sample size.
    > lambda<-14
    > n<-10
    > pop_se<-sqrt(lambda/n)
    > pop_se
     [1] 1.183216
    > x<-NULL
    > for (i in 1:1000) {
    +     x[i] <- mean(rpois(n = n, lambda = lambda))
    + }
    > hist(x, breaks = seq(from = lambda - 4 * sqrt(lambda)/sqrt(n), to = lambda + 
    +     4 * sqrt(lambda)/sqrt(n), length.out = 20), probability = TRUE)
    > sd<-sd(x)
    > sd
     [1] 1.185057
    qqnorm(x)
    qqline(x)
  Code for samples of size 100. The mean stays the same, the distribution is still normal, but the standard deviation/spread of the sampling distribution is lower.
    > n<-100
    > pop_se<-sqrt(lambda/n)
    > pop_se
     [1] 0.3741657 [ETA: this is the same value as the module, though the previous returned values were different]
    > x<-NULL
    > for (i in 1:1000) {
    +     x[i] <- mean(rpois(n = n, lambda = lambda))
    + }
    > hist(x, breaks = seq(from = lambda - 4 * sqrt(lambda)/sqrt(n), to = lambda + 
    + 4 * sqrt(lambda)/sqrt(n), length.out = 20), probability = TRUE)
    > sd<-sd(x)
    > sd
     [1] 0.3727411 [ETA: different from the mod]
    > qqnorm(x)
    > qqline(x)
  Distributions can be converted to standard normals by subtracting off the expected population mean (lambda) and dividing by the standard error of the mean (an estimate of the standard deviation of the sampling distribution) and then plotting a histogram of those values along with a normal curve.
    > curve(dnorm(x, 0, 1), -4, 4, ylim=c(0, 0.8))
    > z<-(x-lambda)/pop_se
    > hist(z, breaks = seq(from = -4, to = 4, length.out = 20), probability = TRUE, add = TRUE)
  CLT in action, using sum() instead of mean():
    > n<-100
    > x<-NULL
    > for (i in 1:1000) {
    +     x[i] <- sum(rpois(n = n, lambda = lambda))
    + }
    > hist(x, breaks = seq(min(x), max(x), length.out = 20), probability = TRUE)
  Both histograms seemed to have a pretty normal distribution.
  
Take Home Points:
  1) CLT states that, regardless, of underlying distribution, the distribution of averages/sums/stdevs/etc based on a large number of independent, identically distributied variables will be approximately normal, centered at the population mean, and will have a standard deviation roughly equal to the standard error of the mean. Also, variables expected to be the sum of multiple independent processes (eg measurement errors) will also have nearly normal distributions.
  2) Mean +/- the relevant standard normal quantile * the standard error equals a CI for the mean, which widens as coverage increases and gets narrower with less variability/larger sample sizes.
  3) As sample size increases, the standard error of the mean decreases and the distribution becomes increasingly normal. Module has a link for a simulation demonstrating the CLT.
  
Confidence Intervals for Sample Proportions
  Discrete binary variables: pi equals the expectation for proportion of successes (proportion of successes = x/n); pi is to proportion is what mu is for mean; the average number of successes across multiple trials.
  Proportion of successes has an normal distribution for expected samples. Standard deviation is estimated by sqrt(pi(1-pi)/n); essentially the standard error of the mean (since it's the expected variance/sample size). If there's no population estimate for pi, we can estimate this from a sample as p^=x/n. This expectation is only true if both n*pi and n*(1-pi) are greater than roughly 5, which chould be checked when working with proportion data.
  Challenge: Elizabeth Warren poll; 856 out of 1000 voters say they'll vote for Warren, how can we characterize the mean and variability?
    > n<-1000
    > x<-856
    > phat<-x/n [our estimate of pi]
    > phat
     [1] 0.856
  Are n*pi and n*(1-pi) both >5? Yes.
    > n*phat
     [1] 856
    > n*(1-phat)
     [1] 144
    > pop_se<-sqrt((phat)*(1-phat)/n)
  What is the 95% CI for this estimate?
    > curve(dnorm(x, mean=phat, sd=pop_se), phat-4*pop_se, phat+4*pop_se)
    > upper<-phat+qnorm(0.975)*pop_se
    > lower<-phat-qnorm(0.975)*pop_se
    > ci<-c(lower, upper)
    > polygon(cbind(c(ci[1], seq(from=ci[1], to=ci[2], length.out=1000), ci[2]), c(0, dnorm(seq(from=ci[1], to=ci[2], length.out=1000), mean=phat, sd=pop_se), 0)), border="black", col="gray")
    > abline(v=ci)
    > abline(h=0)
    
Small Sample Confidence Intervals
  CIs based on CLT and normal distributions take the form of: mean +/- Z(the quantile from the standard normal curve)*standard error of the mean.
  If the sample is small, ie n<30, quantiles are generated by the t distribution. This is typical to encounter, since we often don't have info about the population a sample is drawn from.
  The t distribution is a continuous probability distribution similar in shape to the normal, and is generally used when dealing with stats estimated from a sample instead of known population parameters. Any particular t distribution looks like a normal one; bell-shaped, symmetric, unimodal, and zero-standard (if standardized). Which t distribution to use in a particular stats test is based on the degrees of freedom. The t distribution is essentially a family of curves that approaches the normal curve as the degrees of freedom increases. Low df means fat distribution tails.
  T distribution CIs are: mean +/- T(the quantile from the t distribution)*standard error of the mean.
  Standard normal distributions can be generated by normalizing the sample, ie subtracting the population mean from each observation and dividing all the products by the population standard deviation [(mean(x)-mu)/sigma]. T distributions can be generated by dividing the products by the standard error of the mean [(mean(x)-mu)/SEM]. This takes into account sample size.
  This code plots a standard normal distribution and then t distributions with varying degrees of freedom, specified using the df= argument. Like all distributions, R implements density, cumulative probability, quantile, and random functions.
    > mu<-0
    > sigma<-1
    > curve(dnorm(x, mu, 1), mu-4*sigma, mu+4*sigma, main="Normal Curve=red\nStudent's t=blue", xlab="x", ylab="f(x)", col="red", lwd=3)
    > for (i in c(1, 2, 3, 4, 5, 10, 20, 100)) {
    +     curve(dt(x, df = i), mu - 4 * sigma, mu + 4 * sigma, main = "T Curve", xlab = "x", 
    +           ylab = "f(x)", add = TRUE, col = "blue", lty = 5)
    + }
  The t distribution's fatter tails results in more extreme quantile values for a specific probability than in normal distributions. Therefore, the CIs will also be slightly wider.
  95% CIs for normal distribution:
    > n<-1e+05
    > mu<-3.5
    > sigma<-4
    > x<-rnorm(n, mu, sigma)
    > sample_size<-30
    > s<-sample(x, size=sample_size, replace=FALSE)
    > m<-mean(s)
    > m
     [1] 4.243769
    > sd<-sd(s)
    > sd
     [1] 4.158055
    > sem<-sd(s)/sqrt(length(s))
    > sem
     [1] 0.7591535
    > lower<-m-qnorm(1-0.05/2)*sem 
    > upper<-m+qnorm(1-0.05/2)*sem [(1-alpha)/2 each in the upper and lower tails of the distribution]
    > ci_norm<-c(lower, upper)
    > ci_norm
     [1] 2.755856 5.731683
  95% CIs for t distribution; for sample size 30, the difference is negligible:
    > lower<-m-qt(1-0.05/2, df=sample_size-1)*sem
    > upper<-m+qt(1-0.05/2, df=sample_size-1)*sem
    > ci_t<-c(lower, upper)
    > ci_t
     [1] 2.691126 5.796412
  95% CIs for both with a sample size of 5:
    > sample_size<-5
    > s<-sample(x, size = sample_size, replace = FALSE)
    > m<-mean(s)
    > m
     [1] 5.511681
    > sd<-sd(s)
    > sd
     [1] 5.597979
    > sem<-sd(s)/sqrt(length(s))
    > sem
     [1] 2.503492
    > lower<-m-qnorm(1-0.05/2)*sem
    > upper<-m+qnorm(1-0.05/2)*sem
    > ci_norm<-c(lower, upper)
    > ci_norm
     [1]  0.6049263 10.4184350
    > lower<-m-qt(1-0.05/2, df=sample_size-1)*sem
    > upper<-m+qt(1-0.05/2, df=sample_size-1)*sem
    > ci_t<-c(lower, upper)
    > ci_t
     [1] -1.439128 12.462489