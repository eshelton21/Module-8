Module 8
Probabilities and Distributions
Preliminaries: install the {manipulate} package.
Objectives:
  The objective of this module is to begin our discussion of statistical inference from a frequentist/classical statistics approach. Doing so means that we need to cover basics of probability and distributions.
Recap of Important Terms and Concepts:
  Population = includes all of the elements from a set of data = N
  Sample = one or more observations from a population = n
  Parameter = a measurable characteristic of a population
  Statistic = a measurable characteristic about a sample
  When we do statistical inference we are basically trying to draw conclusions about a population based on measurements from a noisy sample or trying to evaluate whether it is reasonable to assume that our sample is drawn from a particular population. This process of trying to draw conclusions is complicated by the fact that our sample may be biased/non-random/non-representative in some way; there may be unknown or unobserved variable that impact how the sample is related to the population; the assumptions we make about the population that our sample is drawn from might not be correct.
  
Probability
  The term 'probability' is applied to population level variables that describe the magnitude of chance associated with particular observations or events. Probabilities summarize the relative frequencies of possible outcomes, are properties of distributions, and vary between zero and one. Outcomes that are impossible have Pr=0, those that are certain have Pr=1.
  Example: if we roll a fair die, there are six possible outcomes, each has a probability of occurring of 1 in 6. This is referred to as a frequentist or classical way of thinking about the probability of different outcomes; the relative frequency with which an event occurs over numerous identical, objective trials. We will use the {manipulate} package and the sample() function to explore the effects of sample size on estimates of the probability of different outcomes. The probability of each outcome; rolling a 1, 2, etc; is 1 in 6, but our estimate of the probability of each possible outcome will change with sample size.
    library(manipulate)
    outcomes<- c(1,2,3,4,5,6)
    manipulate(
      hist(sample(outcomes, n, replace = TRUE),
        breaks = c(0.5,1.5,2.5,3.5,4.5,5.5,6.5),
        probability = TRUE,
        main = paste("Histogram of Outcomes of ", n, "Die Rolls", sep = " "), 
        xlab = "roll",
        ylab = "probability"),
      n = slider(0, 10000, initial = 100, step = 100)
    ): returned a bar graph
  Write a function to simulate rolling a die where you pass the number of rolls as an argument. Then, use your function to simulate rolling two dice 1000 times and take the sum of the rolls. Plot a histogram of those results.
    nrolls<- 1000: returned several warning messages about doTryCatch(return(expr)...)
    roll<- function(x){sample(1:6, x, replace = TRUE)}
    two_dice<- roll(nrolls)+roll(nrolls)
    hist(two_dice, breaks = c(1.5:12.5), probability = TRUE, main = "Rolling Two Dice", xlab = "sum of rolls", ylab = "probability"): returns a histogram
    
Rules of Probability:
  Pr(+) = probability that something occurs = 1
  Pr(∅) = probability that nothing occurs = 0
  Pr(A) = probability that a particular event A occurs
  0 ≤ Pr(A) ≤ 1
  Pr(A⋃B): probability that a particular event A or a particular event B occurs = UNION
  Pr(A⋂B): probability that both A and B occur simultaneously = INTERSECTION
  Pr(A⋃B) = Pr(A) + Pr(B) - Pr(A⋂B)
    If A and B are mutually exclusive, then this simplifies to Pr(A) + Pr(B)
  Pr(A⋂B) = Pr(A|B)*Pr(B) = Pr(B|A)*Pr(A)
    Pipe operator '|' can be read as 'given'
    If the two events are independent, ie if the probability of one doesn't depend on the probability of the other, then Pr(A⋂B) simplifies to Pr(A)*Pr(B)
    If Pr(A⋂B) = 0, the events are mutually exclusive
  Pr(Ā) = probability of the complement of A = 1 - Pr(A)
  
  Conditional probability is the probability of an even occuring after taking into account the occurrence of another event, ie one even is conditioned on the occurrence of a different event. For example, the probability of a die coming up as '1' given that we know the die came up as an odd number.
  Pr(A|B) = Pr(A⋂B)/Pr(B)
    If A and B are independent, then Pr(A|B) = (Pr(A)*Pr(B))/Pr(B) = Pr(A)
    If A and B are dependent, then Pr(A|B) =/= Pr(A)

Random Variables
  A random variable is a variable whose outcomes are assumed to arise by chance or according to some random or stochastic, randomly determined, mechanism. The chances of observing a specific outcome or an outcome value within a specific interval has associated with it a probability.
  Random variables come in two varieties: 
    Discrete Random Variables are random variables that can assume only a countable number of discrete possibilities, eg counts of outcomes in a particular category. We can assign a probability to each possible outcome.
    Continuous Random Variables are random variables that can assume any real number value within a given range, eg measurements. We cannot assign a specific probability to each possible outcome value as the set of possible outcomes is infinite, but we can assign probabilities to intervals of outcome values.
  A Probability Function is a mathematical function that describes the chance associated with a random variable having a particular outcome or falling within a given range of outcome values. We can also distinguish two types of probability functions.
  Probability Mass Functions (PMFs) are associated with discrete random variables. These functions describe the probability that a random variable takes a particular discrete value. To be a valid PMF, a function f(x) must satisfy the following:
    1) There are k distinct outcomes x1, x2,...xk.
    2) 0 ≤ Pr (X=xi) ≤ 1 for all xi
    3) ∑Pr (X=xi) for all x from x1 to xk = 1
    Flipping a Fair Coin
      outcomes<- c("heads", "tails") ETA: returns six warning messages about doTryCatch
      prob<- c(1/2, 1/2)
      barplot(prob, ylim = c(0,0.6), names.arg = outcomes, space = 0.1, xlab = "outcome", ylab = "Pr(X = outcome)", main = "Probability Mass Function")
      cumprob<- cumsum(prob)
      barplot(cumprob, names.arg = outcomes, space = 0.1, xlab = "outcome", ylab = "Cumulative Pr(X)", main = "Cumulative Probability")
    Rolling a Fair Die
      outcomes<- c(1,2,3,4,5,6)
      prob<- c(1/6,1/6,1/6,1/6,1/6,1/6)
      barplot(prob, ylim = c(0,0.5), names.arg = outcomes, space = 0.1, xlab = "outcome", ylab = "Pr(X = outcome)", main = "Probability Mass Function")
      cumprob<- cumsum(prob)
      barplot(cumprob, names.arg = outcomes, space = 0.1, xlab = "outcome", ylab = "Cumulative Pr(X)", main = "Cumulative Probability")
  Probability Density Functions (PDFs) are associated with continuous random variables. These functions describe the probability that a random variable falls within a given range of outcome values. The probability associated with that range equals the area under a density function for that range. To be a valid PDF, a function f(x) must satisfy the following:
    1) f(x) ≥ 0 for all −∞ ≤ x ≤ +∞. That is, the function f(x) is non-negative everywhere.
    2) ∫+∞−∞ f(x) dx = 1. That is, the total area under the function f(x) = 1.
    The Beta Distribution refers to a family of continuous probability distributions defined over the interval [0,1] and parametrized by two positive shape parameters, denoted by α and β, that appear as exponents of the random variable x and control the shape of the distribution. f(x) = K*(x^(α−1))((1−x)^(β−1)). If we set K = 2, α = 2, and β =1, and restrict the domain of x to [0,1], it gives us a triangular function that we can graph as follows:
      library(ggplot2)
      a<- 2
      b<- 1
      K<- 2
      x<- seq(from = 0, to = 1, by = 0.025)
      fx<- K*x^(a-1)*(1-x)^(b-1)
      lower_x<- seq(from = -0.25, to = 0, by = 0.025): adds some values of x less than zero
      upper_x<- seq(from = 1, to = 1.25, by = 0.025): adds some values of x greater than one
      lower_fx<- rep(0,11): adds fx=0 values to x<0
      upper_fx<- rep(0,11): adds fx=0 values to x>1
      x<- c(lower_x, x, upper_x): pastes x's together
      fx<- c(lower_fx, fx, upper_fx): pastes fx's together
      d<- as.data.frame(cbind(x,fx))
      p<- ggplot(data = d, aes(x = x, y = fx)) + xlab("x") + ylab("f(x)") + geom_line()
      p: returns a triangular graph
    Is this a PDF? Yes, it satisfies both criteria for a PDF: f(x) ≥ 0 for all −∞ ≤ x ≤ +∞ and the total area under f(x) = 1. We can show this interactively:
      library(manipulate)
      manipulate(
        ggplot(data=d, aes(x=x, y=fx)) + xlab("x") +ylab("f(x)") + geom_line() + geom_polygon(data=data.frame(xvals=c(0,n,n,0), fxvals=c(0,K*n^(a-1)*(1-n)^(b-1),0,0)), aes(x=xvals, y=fxvals)) + ggtitle(paste("Area Under Function = ", 0.5 * n * K*n^(a-1)*(1-n)^(b-1), sep = " ")), n=slider(0, 1, initial=0.5, step=0.01)
      )
      The shaded area here represents the cumulative probability integrated across f(x) from -inf to x. ETA: returns the same graphs as line 93, with the left portion of the triangle shaded.
  The cumulative distribution function, or CDF, of a random variable is defined as the probability of observing a random variable X taking the value of x or less, ie F(x) = Pr(X≤x). This definition applies regardless of whether X is discrete or continuous. Note here we are using F(x) for the cumulative distribution function rather than f(X), which we use for the probability density function. For a continuous variable, the PDF is simply the first derivative of the CDF, ie $f(x) = dF(x).
    x<- seq(from = 0, to = 1, by = 0.005)
    prob<- 0.5 * x * K*x^(a-1)*(1-x)^(b-1)
    barplot(prob, names.arg = x, space = 0, main = "Cumulative Probability", xlab = "x", ylab = "Pr(X≤x)"): returns an exponential shaded graph
  The built in R function for the Beta Distribution, pbeta(), can give us the cumulative probability directly, if we specify the values of a=2 and b=1.
    pbeta(0.75,2,1): cumulative probability for x≤0.75, returns 0.5625
    pbeta(0.5,2,1): cumulative probability for x≤0.50, returns 0.25
  In general, we find the cumulative probability for a continuous random variable by calculating the area under the probability density function of interest from -inf to x. This is what is being returned from pbeta(). The other related Beta Distribution functions; eg rbeta(), dbeta(), and qbeta();are also useful. rbeta() draws random observations from a specified beta distribution. dbeta() gives the point estimate of the beta density function at the value of the argument x, and qbeta() is essentially the converse of pbeta(); ie it tells you the value of x that is associated with a particular cumulative probability or quantile, of the cumulative distribution function. Other PMFs and PDFs have comparable r, d, p, and q functions. Note the relationship between the p and q functions:
    pbeta(0.7,2,1): returns 0.49
    qbeta(0.49,2,1): returns 0.7
  We can define the survival function for a random variable X as S(x)=Pr(X>x)=1-Pr(X≤x)=1-f(x). Finally, we can define the 'qth' quantile of a cumulative distribution function as the value of x at which the CDF has the value 'q', ie F(xq)=q.
      
Expected Mean and Variance of Random Variables
  The mean value, or expectation, and the expected variance for a random variable with a given probability mass function can be expressed generally as follows:
    μX = Expectation for X = ∑ xi × Pr (X=xi) for all x from xi to xk
    σ2X = Variance of X = ∑ (xi−μX)2 × Pr (X=xi) for all x from xi to xk
  Applying these formulae to die rolls, we could calculate the expectation for X for a large set of die rolls: (1*1/6)+(2*1/6)+...+(6*1/6) = 3.5
    m<- sum(seq(1:6)*1/6)
    m: returns 3.5
  And the expected variance: [(1-3.5)^2*(1/6)]+...+[(6-3.5)^2*(1/6)]
    var<- sum((seq(1:6)-mean(seq(1:6)))^2*(1/6))
    var: returns 2.916667
  Likewise, we can calculate the expectation and variance for a random variable X with a given probability density function generally as follows: 
    μX = Expectation for X = ∫+∞−∞ x f(x) dx
    σ2X = Variance of X = ∫+∞−∞ (x−μX)2 f(x) dx
  To demonstrate this numerically would require a bit of calculus, ie integration.
  
Useful Probability Distribution for Random Variables
  The Bernoulli Distribution is the probability distribution of a binary random variable, ie a variable that has only two possible outcomes; success or failure, heads or tails, true or false. If p is the probability of one outcome, then 1-p has to be the probability of the alternative. For flipping a fair coin, for example, p = 1-p = 0.5. For the Bernoulli Distribution, the probability mass function is: f(x) = px(1−p)1−x where x = {0 or 1}. For this distribution, μX = p and σ2X = p(1−p).
    Using the Bernoulli Distribution, calculate the expectation for drawing a spade from the deck of cards. What is the variance in this expectation across a large number of draws?
      Pr (spade) = (13/52)^1 × (39/52)^0 = 0.25
      Var (spade) = (13/52) × (1−13/52) = (0.25) × (0.75) = 0.1875
  The Bernoulli distribution is a special case of the Binomial distribution. The binomial distribution is typically used to model the probability of a number of successes, 'k', out of a set of trials, 'n', ie for counts of a particular outcome. Again, the probability of success on each trial = p, and the probability of not success  = 1-p.
  For the Binomial Distribution, the probability mass function is f(x)=(n!/k!(n-k!))*p^k*(1-p)^(n-k). This is read as 'n choose k', ie the probability of k successes out of n trials. This is also called the binomial coefficient. For this distribution, μX = np and σ2X = np(1-p). Recall, μX = expected number of successes in n trials. Where n=1, this simplifies to the Bernoulli distribution.
  What is the chance of getting 1 on each of six consecutive rolls of a die? What about of getting exactly three 1's? What is the expected number of 1's to occur in six consecutive rolls?
    n<-6: number of trials
    k<-6: number of successes
    p<-1/6
    prob<-(factorial(n)/(factorial(k)*factorial(n-k)))*(p^k)*(1-p)^(n-k)
    prob: returns 2.143347e-05
    k<-3
    prob<-(factorial(n)/(factorial(k)*factorial(n-k)))*(p^k)*(1-p)^(n-k)
    prob: returns 0.05358368
  As for other distributions, R has a built in density function, the dbinom() function, that you can use to solve for the probability of a given outcome, ie Pr(X=x).
    dbinom(x=k, size=n, prob=p): returns 0.05358368
  We can also use the built in function pbinom() to return the value of the cumulative distribution function for the binomial distribution, ie the probability of observing up and including a given number of successes in n trials. So, for example, the chances of observing exactly 0, 1, 2, 3,...6 rolls of 1 on 6 rolls of a die are:
    probset<-dbinom(x=0:6, size=6, prob=1/6): x is number of successes, size is number of trials
    barplot(probset, names.arg=0:6, space=0, xlab="outcome", ylab="Pr(X = outcome)", main="Probability Mass Function")
    cumprob=cumsum(probset)
    barplot(cumprob, names.arg=0:6, space=0.1, xlab="outcome", ylab="Cumulative Pr(X)", main="Cumulative Probability")
    sum(probset): returns 1, as it should
  The chance of observing exactly 3 rolls of 1 is:
    dbinom(x=3, size=6, prob=1/6): returns 0.05358368
  And the chance of observing up to and including 3 rolls of 1 is:
    pbinom(q=3, size=6, prob=1/6): note the argument name is q not x, returns 0.991298
  This can be calculated by summing the relevant individual outcome probabilities:
    sum(dbinom(x=0:3, size=6, prob=1/6)): this sums up the probabilities of 0, 1, 2, and 3 successes, returns 0.991298
  The probability of observing more than 3 rolls of 1 is given as:
    1-pnbinom(q=3, size=6, prob=1/6): returns 0.9988642
    pnbinom(q=3, size=6, prob=1/6, lower.tail=FALSE): returns 0.9988642
  The probability of observing 3 or more rolls of 1 is:
    1-pbinom(q=2, size=6, prob=1/6): note the q argument is 2, returns 0.06228567
    pbinom(q=2, size=6, prob=1/6, lower.tail=FALSE): returns 0.06228567
  The Poissin Distribution is often used to model open ended counts of independently occuring events, for example the number of cars that pass a traffic intersection over a given interval of time or the number of times a monkey scratches itself during a given observation interval. The probability mass function for the Poisson distribution is described by a single parameter, λ, where λ can be interpreted as the mean number of occurrences of the event in the given interval. The probability mass function for the Poissin Distribution is f(x)=(λ^x*exp(-λ)/x!), where x={0,1,2...}. For this distribution, ux=λ and σ^2_X=λ. Note that the mean and the variance are the same. Let's use R to look at the probability mass functions for a different values of λ:
    x<-0:10
    l=3.5
    probset<-dpois(x=x,lambda=l)
    barplot(probset, names.arg=x, space=0, xlab="x", ylab="Pr(X=x)", main="Probability Mass Function"): returns a bar graph
    x<-0:20
    l=10
    probset<-dpois(x=x, lambda=l)
    barplot(probset, names.arg=x, space=0, xlab="x", ylab="Pr(X=x)", main="Probability Mass Function"): returns a bar graph
    x<-0:50
    l=20
    probset<-dpois(x=x, lambda=l)
    barplot(probset, names.arg=x, space=0, xlab="x", ylab="Pr(X=x)", main="Probability Mass Function"): returns a bar graph
  As we did for other distributions, we can also use the built in probability function for the Poisson distribution, ppois(), to return the value of the cumulative distribution function, ie the probability of observing up to and including a specific number of events in the given interval.
    x<-0:10
    l<-3.5
    barplot(ppois(q=x,lambda=l),ylim=0:1,space=0, names.arg=x,xlab="x",ylab="Pr(X ≤ x)",main="Cumulative Probability")
    x<-0:20
    l<-10
    barplot(ppois(q=x,lambda=l),ylim=0:1,space=0, names.arg=x,xlab="x",ylab="Pr(X ≤ x)",main="Cumulative Probability")
    x<-0:50
    l<-20
    barplot(ppois(q=x, lambda=l), ylim=0:1, space=0, names.arg=x, xlab="x", ylab="Pr(X≤x)", main="Cumulative Probability")

Probability Density Function
  The Uniform Distribution is the simplest probability density function describing a continuous random variable. The probability is uniform and does not fluctuate across the range of x values in a given interval. The probability density function for the Uniform Distribution is f(x)=1/(b-a) where a≤x≤b and 0 for x<a and x>b.
  What would you predict the expectation/mean should be for a uniform distribution? For this distribution: ux=(a+b)/2 and σ^2_X=(b-a)^2/12. Let's plot a uniform distribution across a given range, from a=4 to b=8:
    a<-4
    b<-8
    x<-seq(from=a-(b-a), to=b+(b-a), by=0.01)
    fx<- dunif(x, min=a, max=b): dunif() evaluates the density at each x
    plot(x, fx, type="l", xlab="x", ylab="f(x)", main="Probability Density Function")
  Note that for the uniform distribution, the cumulative density function increases linearly over the given interval.
    plot(x, punif(q=x, min=a, max=b), type="l", xlab="x", ylab="Pr(X≤x)", main="Cumulative Probability"): punif() is the cumulative probability density up to a given x
  Normal distribution: simulate a sample of 10000 random numbers frm a uniform distribution in the interval between a=6 and b=8. Calculate the mean and variance of this simulated sample and compare it to the expectation for these parameters.
  The Normal or Gaussian Distribution is perhaps the most familiar and most commonly applied probability density functions for modeling continuous random variables. Why is the normal so important? Many traits are normally distributed, and the additive combination of many random factors is also commonly normally distributed. 
  Two parameters, u and σ, are used to describe a normal distribution. We can get an idea of the shape of a normal distribution with different u or σ using the simple R code below. Try playing with u and σ.
    mu <- 4
    sigma <- 1.5
    curve(dnorm(x,mu,sigma), mu - 4*sigma, mu + 4*sigma, main="Normal Curve", xlab="x", ylab="f(x)")
  The function dnorm() gives the point value of the normal density function at a given value of x. 'x' can range from -inf to +inf. Recall, it does not make sense to talk about the 'probability' associated with a given value of x as this is a density not a mass function, but we can talk about the probability of x falling within a given interval. The code below lets you play interactively with u, σ, and nsigma, which shades in the proportion of the distribution falling within that number of standard deviations of the mean. Also, look carefully at the code to try to figure out what each bit is doing.
    manipulate(
    plot(seq(from=(mu-4*sigma), to=(mu+4*sigma), length.out=1000),
             dnorm(seq(from=(mu-4*sigma), to=(mu+4*sigma), length.out=1000),mean=mu,sd=sigma),
             type="l", xlim=c(mu-4*sigma, mu+4*sigma), xlab="x", ylab="f(x)",
             main="Normal Probability Density Function")+
            polygon(rbind(c(mu-nsigma*sigma, 0), cbind(seq(from=(mu-nsigma*sigma), to=(mu+nsigma*sigma), length.out=1000), dnorm(seq(from=(mu-nsigma*sigma), to=(mu+nsigma*sigma), length.out=1000),mean=mu,sd=sigma)), c(mu+nsigma*sigma, 0)), border=NA, col="salmon")+        abline(v=mu, col="blue")+abline(h=0)+abline(v=c(mu-nsigma*sigma, mu+nsigma * sigma), col="salmon"),
    mu=slider(-10, 10, initial=0, step=0.25),
    sigma=slider(0.25, 4, initial=1, step=0.25),
    nsigma=slider(0,4,initial=0,step=0.25)
)
  The pnorm() function, as with the p- variant function for the other distributions, returns the cumulative probability of observing a value less than or equal to x, ie Pr(X≤x). Type in the code below and then play with values of u and σ to look at how the cumulative distribution function changes.
    manipulate(
    plot(seq(from=(mu-6*sigma), to=(mu+6*sigma), length.out=1000),
             pnorm(seq(from=(mu-6*sigma), to=(mu+6*sigma), length.out=1000),mean=mu,sd=sigma),
             type="l", xlim=c(-20,20), xlab="x", ylab="f(x)",
             main="Cumulative Probability"),
    mu=slider(-10, 10, initial=0, step=0.25),
    sigma=slider(0.25, 10, initial=1, step=0.25)
    ): plots the cumulative distribution function
  You can also use pnorm() to calculate the probability of an observation drawn from the population falling within a particualr interval. For example, for a normally distributed population variable with u=6 and σ=2, the probability of a random observation falling between 7 and 8 is:
    p<-pnorm(8, mean=6, sd=2)-pnorm(7, mean=6, sd=2)
    p: returns 0.1498823
  Likewise, you can use pnorm() to calculate the probability of an observation falling, for example withing two standard deviations of the mean of a particular normal distribution.
    mu<-0
    sigma<-1
    p<-pnorm(mu+2*sigma, mean=mu, sd=sigma)-pnorm(mu-2*sigma, mean=mu, sd=sigma)
    p: returns 0.9544997
  Regardless of the specific values of u and σ, about 95% of the normal distribution falls within two standard deviations of the mean and about 68% of the distribution falls within one standard deviation.
    p<-pnorm(mu+1*sigma, mean=mu, sd=sigma)-pnorm(mu-1*sigma, mean=mu, sd=sigma)
    p: returns 0.6826895
  Another one of the main functions in R for probability distributions, the qnorm() function, will tell us the value of x below which a given proportion of the cumulative probability function falls. As we saw earlier, too,we can use qnorm() to calculate confidence intervals.
    manipulate(plot(seq(from=(mu - 4*sigma), to=(mu + 4*sigma), length.out = 1000), dnorm(seq(from=(mu - 4*sigma), to=(mu + 4*sigma), length.out = 1000), mean=mu,sd=sigma), type="l",xlim=c(mu - 4*sigma,mu+4*sigma), xlab="x",ylab="f(x)", main="Normal Probability Density Function") + abline(v=mu, col="blue") + abline(h=0) + polygon(x=c(qnorm((1-CI)/2,mean=mu,sd=sigma),qnorm((1-CI)/2,mean=mu,sd=sigma), qnorm(1-(1-CI)/2,mean=mu,sd=sigma),qnorm(1-(1-CI)/2,mean=mu,sd=sigma)),y=c(0,1,1,0),border="red"), mu=slider(-10, 10, initial=0, step=0.25), sigma=slider(0.25, 10, initial=1, step=0.25), CI=slider(0.50,0.99,initial=0.90, step=0.01)
  Challenge: create a vector, v, containing n random numbers selected from a normal distribution with mean u and standard deviation σ. Use 1000 for n, 3.5 for u, and 4 for σ. Hint: such a function exists: rnorm(). Calculate the mean, variance, and standard deviation for your sample of random numbers. Plot a histogram of your random numbers.
    n<-1000
    mu<-3.5
    sigma<-4
    v<-rnorm(n, mu, sigma)
    mean(v): returns 3.609431 [ETA: 3.663831]
    var(v): returns 15.88684 [ETA: 16.90551]
    sd(v): returns 3.98583 [ETA: 4.111631]
    hist(v, breaks=seq(from=-15, to=20, by=0.5), probability=TRUE)
  A quantile-quantile or'Q-Q' plot can be used to look at whether a set of data seem to follow a normal distribution. A Q-Q plot is a graphical method for generally comparing two probability distributions. To examine a set of data for normality graphically, you plot the quantiles for your actual data, as the y values, versus the theoretical quantiles, as the x values, pulled from a normal distribution. If the two distributions being compared are similar, the points on the plot will approximately lie on the y=x slope. 
  In this case, this should be apparent since you have simulated a vector of data from a normal distribution. To quickly do a Q-Q plot, call the two R functions qqnorm() and qqline() using a vector of data you want to examine as an argument.
    qqnorm(v, main="Normal QQ plot random normal variables")
    qqline(v, col="gray"): [ETA: adds a line of best fit]
  This is the same as doing the following: 1) Generate a sequence of probability points in the interval frm 0 to 1 equivalent in length to vector v. 2) Calculate the theoretical quantiles for this set of probabilities based on the distribution you want to compare to; in this case, the normal distribution. 3) Calculate the quantiles for your set of observed data for the same number of points. 4) Plot these qualities against one another.
    p<-ppoints(length(v))
    head(p): returns 0.0005 0.0015 0.0025 0.0035 0.0045 0.0055
    tail(p): returns 0.9945 0.9955 0.9965 0.9975 0.9985 0.9995
    theoretical_q<-qnorm(ppoints(length(v)))
    observed_q<-quantile(v, ppoints(v))
    plot(theoretical_q, observed_q, main="Normal QQ plot random number variables", xlab="Theoretical Quantiles", ylab="Sample Quantiles")
  The 'Standard Normal' Distribution: any normal distribution with mean u and standard deviation σ can be converted into what is called the standard normal distribution, where the mean in zero and the standard deviation is 1. This is done by subtracting the mean from all observations and dividing these differences by the standard deviation. The resultant values are referred to as Z scores, and they reflect the number of standard deviations an observation is from the mean.
    x<-rnorm(10000, mean=5, sd=8): simulate from a normal distribution with mean 5 and sd 8
    hist(x)
    mean(x): returns 5.002401 [ETA: 5.115181]
    sd(x): returns 8.067177 [ETA: 7.997908]
    z<-(x-mean(x))/sd(x): standardized!
    hist(z)
    mean(z): returns 1.916109e-17 [ETA: -4.882e-19]
    sd(z): 1
    
Sample Distributions versus Population Distributions
  It is important to recognize that, above, we were dealing with probability distributions of discrete and continuous random variables as they relate to populations. But, as we have talked about before, we almost never measure entire populations; instead, we measure samples from populations and we characterize our samples using various statistics. The theoretical probability distributions described above, and others, are models for how we connect observed sample data to populations, taking into account various assumptions, and this is what allows us to do many types of inferential statistics. The most fundamental asusmption is that the observations we make are independent from one another and are identically distributed, an assumption often abbreviated as 'iid'. Obvious cases of violation of this assumption are rife in the scientific literature, and we should always be cautious about making this assumption.
  The important thing for us to know is that we can get unbiased estimates of population level parameters on the basis of sample statistics. Let's imagine a population of one million zombies whose age at zombification is characterized by a normal distribution with a mean of 25 years and a standard deviation of 5 years.
    set.seed(1)
    x<-rnorm(1000000, 25, 5)
    hist(x, probability=TRUE)
    mu<-mean(x)
    mu: returns 25.00023
    sigma<-sqrt(sum((x-mean(x))^2)/length(x))
    Note: we don't use the sd() function as this would divide by length(x)-1.
  Suppose we now sample the zombie population by trapping sets of zombies and determining the mean age in each set. We sample without replacement from the original population for each set. Let's do that 100 times with samples of size 5 and store these in a list.
    k<-1000: number of samples
    n<-5: size of each sample
    s<-NULL: dummy variable to hold each sample
    for (i in 1:k){
      s[[i]] <- sample(x,size=n, replace=FALSE)
    }
    head(s): returns 6 sets of 5 numbers
  For each of these samples, we can then calculate a mean age, which is a statistic describing each sample. That statistic itself is a random variable with a mean and distribution. This is the sampling distribution. How does the sampling distribution compare to the population distribution? The mean of the two is pretty close to the same. The sample mean, which is an average of the set of sample averages, is an unbiased estimator for the population mean.
    m<-NULL
    for (i in 1:k){
      m[i] <- mean(s[[i]])
    }
    mean(m): returns 25.05822
    mu: 25.00023
  Again, this is the mean of the sampling distributioin, which is simply the average of the means of each sample. This value should be really close to the population mean.

The Standard Error
  The variance in the sampling distribution, ie of all possible means of samples of size n from a population, is σ^2/n. The square root of this variance is the standard deviation of the sampling distribution, also referred to as the standard error. Thus, is the population variance σ^2, and thus the population standard deviation σ, is known, then the standard error can be calculated as square root of (σ^2/n), or equivalently, σ/(square root of the sample size).
    pop_se<-sqrt(sigma^2/n)
    pop_se: returns 2.236481
    pop_se<- sigma/sqrt(n)
    pop_se: returns 2.236481
  If the true population standard deviation is not known, the standard error can still be estimated from the standard deviaiton of any given sample. Thus, analogous to the formula we used when the true population standard deviation was known, the standard error calculated from a sample is simply the sample standard deviation/(square root of the sample size).
    stdev<-NULL
    for (i in 1:k){
      stdev[i] <- sd(s[[i]])
    }
    sem<-stdev/sqrt(n): a vector of SEs estimated from each sample
    head(sem): returns 2.141974 2.995999 1.200017 2.346671 3.094917 1.391291
    mean(sem): returns 2.113435
    pop_se: returns 2.236481
  Thus, the standard error of the mean calculated from an individual sample can be used as an estimator for the standard deviation of the sampling distribution. This is extremely useful, since it means that, if our sample is large enough, we don't have to repeatedly sample from the population to get an estimate of the sampling distribution directly using our data. Note that as our sample size increases, the standard error of the mean should decrease, as should the standard deviation in estimates of the population mean drawn from successive samples. This should be apparent intiutively, as each sample drawn from a popultion gets larger, the estimate of the mean value of those samples should vary less and less. Despite their similarities, the standard error of the mean calculated for a given sample and the standard deviaiton of that given sample tell us different things. The standard error of the mean is an estimate of how far a given sample mean is likely to be from the population mean, it is a measure of uncertainty. The standard deviation of a sample is a measure of the degree to which individual values within a sample differ from the sample mean.
  
Homework: Zombie Data
  library(curl)
  f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall19/zombies.csv")
  d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
  1) Calculate population mean and standard deviation for each quantitative random variable. Note: var() and sd() commands are for samples, not populations.  
    [No idea.]
  2) Use {ggplot} to make boxplots of each of these variables by gender. [Mod 7 notes]
    library(ggplot2)
  3) Use {ggplot} and make scatterplots of height and weight in relation to age. Do these variables seem to be related? In what way? [Mod 7 notes]
  4) Using histograms and Q-Q plots, check whether the quantitative variables seem to be drawn from a normal distribution. Which seem to be and which do not? For those that are not, can you determine what common distribution they're drawn from?
  5) Now use the sample() function to sample one subest of 30 zombie surviviors, without replacement, from this population and calculate the mean and sample standard variation for each variable. Also estimate the standard error for each variable and construct the 95% confidence interval for each mean. Note that for the variables that are not drawn from the normal distribution, you will need to base your estimate of the CIs on some different distribution. [Mod 8, start of Mod 9]
  6) Now draw 99 more random samples of 30 survivors out and calculate the mean for each. You now have a total of 100 means for each variable, each based on 30 observations, which constitutes a sampling distribution for each variable. What are the means and standard deviations of this for each variable? How do the standard deviations compare to the standard errors estimated in 5? What do these sampling distributions look like? Are they normally distributed? What about for those variables that you concluded were not originally drawn from a normal distribution?